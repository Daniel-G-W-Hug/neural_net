\subsection{Neural Net Definitions}

Here we look into a neural network based on perceptron cells with an activation function.
A single perception can be regarded as a linear classifier, i.e. it is able to output
which of two linearly separable data sets a given point belongs to. In order to classify
more complex data sets, layers of perceptrons with non-linear activation functions are
required.

\begin{figure}[h] \centering \includegraphics[width=0.75\textwidth]{neural_network}
    \caption{A fully connected multi-layer neural network with three inputs, two hidden
    layers, each with four perceptron nodes and an output layer with a single output node.
    (Source: \url{https://towardsdatascience.com})} \label{fig:neural_network}
\end{figure}

A neural net consists of $L$ layers with $n^l$ nodes in each layer $l=[0,L)$. For the
input layer $(l=0)$ the nodes are simple input nodes, while for the remaining layers
consist of perceptron nodes (see figure~\ref{fig:percptron}). \\

A perceptron with an arbitrary activation function looks like this:
\begin{figure}[h] \centering \includegraphics[width=0.75\textwidth]{single_neuron}
    \caption{A node of a neural network with inputs, bias and activiation function.
    (Source: \url{https://towardsai.net})}
    \label{fig:percptron}
\end{figure}

Definitions:
\begin{enumerate}
    \item The training dataset $X$ consisting of input-output pairs $(\vec{x}_i,
    \vec{y}_i)$, where $\vec{x}_i$ is the input and $\vec{y}_i$ is the desired or target
    output of the network on the input $\vec{x}_i$. The set of input-output training pairs
    of size $N$ is denoted $X = \{(\vec{x}_1,\vec{y}_1), \dots, (\vec{x}_N,\vec{y}_N)\}$.

    \item A feedforward neural network has (learning) parameters, i.e.\ weights and
    biases, that are collectivly denoted $\theta$. The nodes in different layers are fully
    connected, while there are no connections between nodes in the same layer. For
    backpropagation the parameters of primary interest are $w^l_{tf}$, the
    weight\footnote{In this notation the index $t$ stands for \emph{to}, whereas $f$
    stands for \emph{from}. Index $t$ always refers to nodes in layer $l$, while index $f$
    always refers to nodes in layer $l-1$.} between the node with index $t$ in layer $l$
    and the node with index $f$ in layer $l-1$, and $b^l_t$, the bias of the node with
    index $t$ in layer $l$. 

    \item A loss function $L(X,\theta)$, which defines a quantitative measure for the
    error between the desired output $\vec{y}_i$ and the calculated actual output
    $\vec{a}^l_i$ of the neural network on the input $\vec{x}_i$ for an input pair
    $(\vec{x}_i, \vec{y}_i) \in X$ and a particular value of the learning parameters
    $\theta$.
\end{enumerate}

Following values are used to define the network:
\begin{itemize}
    \item $x_t$, the input at index $t$ of an input vector $\vec{x}_i$ of the training set
    in the input layer ($x_t = a^{l=0}_t$ after activation of the input layer in the
    implementation, see below for definition of $a^l_t$).

    \item $w^l_{tf}$, the weight between the node with index $t$ (\emph{\underline{t}o})
    in layer $l$ and the node with index $f$ (\emph{\underline{f}rom}) in layer $l-1$.

    \item $b^l_t$, the bias of the node with index $t$ in layer $l$.
    
    \item $z^l_t = \sum_{f}{w^l_{tf} a^{l-1}_f} + b^l_t$ is the total input the node gets
    from the activated nodes of the previous layer and it's bias.
    
    \item Activation functions $f_h$ for the \underline{h}idden layers and the
    \underline{o}utput layer $f_o$.
    
    \item The activation $a^l_t$ at index $t$ in layer $l$. $a^l_t = f_h(z^l_t)$ in hidden
    layers or $a^{L-1}_t = f_o(z^{L-1}_t)$ for the output layer. The activation function
    for the input layer is the identity function in the implementation.

    \item $a^{L-1}_t$ is the output component at the index $t$ in the output layer at the
    given input $\vec{x}_i$. The intended output is defined by $\vec{y}_i$ as second part
    of the training pair $(\vec{x}_i, \vec{y}_i)$, where $y_t$ is component $t$ of
    $\vec{y}_i$.
\end{itemize}

The goal of training is to minimize the loss function and thus to minimize the remaining
error for a given set of training data.  A typical example of a loss function is the
quadratic cost function (mean squared error with additional factor of $\frac{1}{2}$ for
easy calculation of its derivative). The total loss is a sum of the partial losses $L_i$
for each training pair $(\vec{x}_i, \vec{y}_i)$ as shown in
equation~(\ref{eq:mse_loss_func}), where the partial loss itself is a sum over the
contributions of each component $t$ of the difference between the actual output vector of
the network and the intended output vector for a given training pair $i$.

\begin{equation}
    L(X,\theta) = \frac{1}{2N} \sum_{i=1}^{N}{\lVert \vec{a}^{L-1}_i - \vec{y}_i \rVert^2}
    = \frac{1}{2N} \sum_{i=1}^{N}{\underbrace{\sum_{t}{(a^{L-1}_t - y_t)^2}}_{L_i}}
    \label{eq:mse_loss_func}
\end{equation}

We need to understand how the deviation of the actual output to the desired output depends
on the choice of learning parameters $\theta$, i.e. the weights $w$ and the bias values
$b$. In order to minimize the the loss function, and thus the deviation between acutal and
desired output, gradient descent is used. This means that the training parameters have to
be changed in a direction against the gradient of the function $L$. This requires to
calculate all partial derivatives of the loss function with respect to the weights
$\pd{L}{w}$ and with respect to the biases $\pd{L}{b}$ and update each learning parameter
according to

\begin{eqnarray}
    w^{new} & = &  w^{old} - \eta \pd{L}{w} \\
    b^{new} & = &  b^{old} - \eta \pd{L}{b}
    \label{eq:update_weights_and_bias}
\end{eqnarray}

with $\eta$ as learning rate.\\

To understand why this is the case, let's consider an arbitrary cost function with three
parameters $C(w_1,w_2,b)$ as an example. For small changes we get
\begin{equation}
    \Delta C \approx \pd{C}{w_1}\Delta w_1 + \pd{C}{w_2}\Delta w_2 +\pd{C}{b}\Delta b
\end{equation}
So $\Delta C \approx \vec{\nabla} C \cdot \Delta \vec{v}$ with $\vec{\nabla} C =
(\pd{C}{w_1}, \pd{C}{w_2}, \pd{C}{b})^T$ and $\Delta \vec{v} = (\Delta w_1, \Delta w_2,
\Delta b)^T$.\\

To minimize C, we need to assure that $\Delta C < 0$. \\

If we specifically choose $\Delta \vec{v} = -\eta \vec{\nabla} C$, i.e. select the change
of $\Delta \vec{v}$ against the direction of the gradient vector, we get
\begin{equation}
    \Delta C \approx \vec{\nabla} C \cdot (-\eta \vec{\nabla} C)
    = -\eta \vec{\nabla} C \cdot \vec{\nabla} C
    = -\eta \lVert \vec{\nabla} C \rVert ^2
\end{equation}
which gives us a negative $\Delta C$, because $\lVert \vec{\nabla} C \rVert ^2$ is a
positive value. \\

In practise we can either calculate the gradient for the full training set, or rather
approximate the gradient by selecting a random subset of the training samples (called
mini-batch), or directly calculate the gradient we get for each individual training sample
pair (so-called stochastic gradient descent), before we apply the resulting change to the
weights and biases. These approaches reduce the computational effort and typically
influence the learning speed. \\

It is called an epoch, when the neural net has seen all available training data once
during a training cycle. The training typically covers several hundred epochs for
minimizing the loss function. \\

The available data is normally split into training data, validation data, test data in a
ratio of 70:20:10. The training data set is used to learn the training parameters. The
validation data is used for optimizing the meta parameters (e.g. learning rate, batch
sizes, activation functions, etc.). The test data set is used to calculate the quality of
the training as a result (precision, recall, accuracy). All subsets must of course be
large enough to be representative for the full available data set or we run into trouble
due to biased data in the subsets. \\

\newpage
