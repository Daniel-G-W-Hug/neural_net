# network structure (vector<int>, i.e. comma separated int > 0):
# (number of nodes per layer)
2, 2, 1
# activation function for hidden layers (int):
# (identity=1, sigmoid=2, tanhyp=3, reLU=4, leaky_reLU=5)
2
# activation function for output layer (int):
# (identity=1, sigmoid=2, tanhyp=3, reLU=4, leaky_reLU=5)
2
# epochmax = number of iteration over the whole data set (int > 0):
5000
# limit output to every nth iteration in each epoch (int > 0):
# (epoch_output_skip, such that epoch%epoch_output_skip == 0):
500
# learning_rate for gradient descent (double > 0.0):
0.1
# min_target_loss = minimum loss in training to stop prescribed interation,
# even if epoch < epochmax (double > 0.0):
1.e-3
# min_relative_loss_change_rate = minimum relative loss change rate per epoch (double > 0.0):
# (stop prescribed interation, even if epoch < epochmax)
1.e-7
# update_strategy = update after each training pair or after batch of training pairs (int):
# (immediate_update=1, mini_batch_update=2, full_batch_update=3 )
1
# mini_batch_size = no. of randomly selected samples from full training set for gradient calc. (int > 0):
# (only used for update strategy "mini_batch_update", otherwise ignored)
2